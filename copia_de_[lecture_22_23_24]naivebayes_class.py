# -*- coding: utf-8 -*-
"""Copia de [Lecture_22/23/24]NaiveBayes_class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/190QWhcH1qiLAPCzvLZ9UM5KfR8zp--Fy
"""

import math
import os

os.listdir('corpus1/spam')

"""## Preparación del corpus de emails"""

!git clone https://github.com/pachocamacho1990/datasets

! unzip datasets/email/plaintext/corpus1.zip

data = [] #Lista de datos
clases = [] #Lista de clases
#lectura de spam data
for file in os.listdir('corpus1/spam'): #Recorrer lista de archivos
  with open('corpus1/spam/'+file, encoding='latin-1') as f: #Va a abrir cada archivo y con caracteres que no identifique
    data.append(f.read()) #Agrega el texto del archivo
    clases.append('spam') #Agrega la clase
#lectura de ham data
for file in os.listdir('corpus1/ham'): #Recorrer lista de archivos
  with open('corpus1/ham/'+file, encoding='latin-1') as f: #Va a abrir cada archivo y con caracteres que no identifique
    data.append(f.read()) #Agrega el texto del archivo
    clases.append('ham') #Agrega la clase
len(data) #Cantidad de datos

"""## Construcción de modelo Naive Bayes

### Tokenizador de Spacy

* Documentación: https://spacy.io/api/tokenizer
* ¿Cómo funciona el tokenizador? https://spacy.io/usage/linguistic-features#how-tokenizer-works
"""

from spacy.tokenizer import Tokenizer
from spacy.lang.en import English

nlp = English()
tokenizer = Tokenizer(nlp.vocab) #Va tokenizar basado en el algoritmo preconstruido de nlp (inglés)

print([t.text for t in tokenizer(data[0])])

"""### Clase principal para el algoritmo

Recuerda que la clase más probable viene dada por (en espacio de cómputo logarítmico):


$$\hat{c} = {\arg \max}_{(c)}\log{P(c)}
 +\sum_{i=1}^n
\log{ P(f_i \vert c)}
$$

Donde, para evitar casos atípicos, usaremos el suavizado de Laplace así:

$$
P(f_i \vert c) = \frac{C(f_i, c)+1}{C(c) + \vert V \vert}
$$

siendo $\vert V \vert$ la longitud del vocabulario de nuestro conjunto de entrenamiento.
"""

import numpy as np

class NaiveBayesClassifier(): #La clase
  nlp = English()
  tokenizer = Tokenizer(nlp.vocab)

  def tokenize(self, doc): #Corresponde al tokenizador
    return  [t.text.lower() for t in tokenizer(doc)]

  def word_counts(self, words): #Contador de palabras
    wordCount = {} #Con diccionario vacio
    for w in words: #Recorrer lista de palabras
      if w in wordCount.keys(): #Verificar si la palabra esta en el diccionario
        wordCount[w] += 1 #De ser así, al conteo de doble le sumas una cuenta adicional
      else:
        wordCount[w] = 1 #De lo contrario creas la llave
    return wordCount #Devuelve el conteo de palabras

  def fit(self, data, clases): #admitir los datos de entrenamiento
    n = len(data) #Cantidad de datos
    self.unique_clases = set(clases) #Conjuntos de clases (las únicas)
    self.vocab = set() #Vocabulario
    self.classCount = {} #C(c) cuantas veces aparece la categoria
    self.log_classPriorProb = {} #P(c)
    self.wordConditionalCounts = {} #C(w|c) conteo
    #conteos de clases
    for c in clases: #Recorrer lista de clases
      if c in self.classCount.keys(): #Verificar si la clase esta en el diccionario
        self.classCount[c] += 1 #De ser así, al conteo de doble le sumas una cuenta adicional
      else:
        self.classCount[c] = 1 #De lo contrario creas la llave
    # calculo de P(c)
    for c in self.classCount.keys(): #Recorrer lista de clases
      self.log_classPriorProb[c] = math.log(self.classCount[c]/n) #Numero de veces de la clase
      self.wordConditionalCounts[c] = {} #Diccionario vacio
    # calculo de C(w|c)
    for text, c in zip(data, clases):
      counts = self.word_counts(self.tokenize(text))
      for word, count in counts.items():
        if word not in self.vocab:
          self.vocab.add(word)
        if word not in self.wordConditionalCounts[c]:
          self.wordConditionalCounts[c][word] = 0.0
        self.wordConditionalCounts[c][word] += count

  def predict(self, data): #Predecir
    results = [] #Lista de resultados
    for text in data: #Recorrer lista de datos
      words = set(self.tokenize(text)) #Lista de palabras
      #calculo de P(w|c)
      scoreProb = {} #Diccionario de probabilidades
      for word in words: #Recorrer lista de palabras
        if word not in self.vocab: continue #ignoramos palabras nuevas
        #suavizado Laplaciano para P(w|c)
        for c in self.unique_clases: #Recorrer lista de clases
          log_wordClassProb = math.log(
              (self.wordConditionalCounts[c].get(word, 0.0)+1)/(self.classCount[c]+len(self.vocab)))
          scoreProb[c] = scoreProb.get(c, self.log_classPriorProb[c]) + log_wordClassProb
      arg_maxprob = np.argmax(np.array(list(scoreProb.values())))
      results.append(list(scoreProb.keys())[arg_maxprob])
    return results

"""### Utilidades de Scikit Learn
* `train_test_split`: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html

* `accuracy_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html

* `precision_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html

* `recall_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score
data_train, data_test, clases_train, clases_test = train_test_split(data, clases, test_size=0.10, random_state=42)

classifier = NaiveBayesClassifier()
classifier.fit(data_train, clases_train)

clases_predict = classifier.predict(data_test)

accuracy_score(clases_test, clases_predict)

precision_score(clases_test, clases_predict, average=None, zero_division=1)

recall_score(clases_test, clases_predict, average=None, zero_division=1)