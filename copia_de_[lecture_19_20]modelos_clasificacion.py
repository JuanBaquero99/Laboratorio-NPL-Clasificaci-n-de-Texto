# -*- coding: utf-8 -*-
"""Copia de [Lecture_19/20]Modelos_clasificacion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1INm2Zi6_xHxyWVIEoYXDNNmnduDhmsMx

# Clasificación de palabras (por género de nombre)
"""

import nltk, random
nltk.download('names')
from nltk.corpus import names

"""**Función básica de extracción de atributos**"""

# definición de atributos relevantes
def atributos(palabra): #La función que recibe palabra
	return {'ultima_letra': palabra[-1]} #por cada palabra devuelve la ultuma letra (-1)

tagset = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')]) #La lista de tuplas con el nombre en español y el genero. Por cada nombre las palabras que contiene

tagset[:10]

random.shuffle(tagset) #Esto nos permite una mezcla (random) para que muestre aleatoriamente masculinos y feneminos sin sesgos
tagset[:10]

fset = [(atributos(n), g) for (n, g) in tagset] #Crea una lista leyendo los elementos de la lista anterior
train, test = fset[500:], fset[:500] #La división en etrenamiento (train) y test(test)

"""**Modelo de clasificación Naive Bayes**"""

# entrenamiento del modelo NaiveBayes
classifier = nltk.NaiveBayesClassifier.train(train) #Aqui se hace el clasificador

""" **Verificación de algunas predicciones**"""

classifier.classify(atributos('amanda')) #Se pasa como atributo

classifier.classify(atributos('peter')) #Se pasa como atributo

"""**Performance del modelo**"""

print(nltk.classify.accuracy(classifier, test)) #Es el acurracy. La certeza

print(nltk.classify.accuracy(classifier, train))

"""**Mejores atributos**"""

def mas_atributos(nombre): #Para mejorar el accuarcy mejores atributos
    atrib = {} #Un diccionario vacio
    atrib["primera_letra"] = nombre[0].lower() #la primera letra desde la posición en el primer caracter en miniscula
    atrib["ultima_letra"] = nombre[-1].lower()
    for letra in 'abcdefghijklmnopqrstuvwxyz': #por todas las letras influye en el atributo
        atrib["count({})".format(letra)] = nombre.lower().count(letra) #El numero de veces que aparece la letra
        atrib["has({})".format(letra)] = (letra in nombre.lower()) #Pasara la letra en el atributo y verificar si la tiene
    return atrib

mas_atributos('jhon')

fset = [(mas_atributos(n), g) for (n, g) in tagset] #Lista de tuplas para recorrer el datset con los más atributos
train, test = fset[500:], fset[:500] #Comparativa del modelo anterior
classifier2 = nltk.NaiveBayesClassifier.train(train)

print(nltk.classify.accuracy(classifier2, test))

"""### Ejercicio de práctica

**Objetivo:** Construye un classificador de nombres en español usando el siguiente dataset:
https://github.com/jvalhondo/spanish-names-surnames

1. **Preparación de los datos**: con un `git clone` puedes traer el dataset indicado a tu directorio en Colab, luego asegurate de darle el formato adecuado a los datos y sus features para que tenga la misma estructura del ejemplo anterior con el dataset `names` de nombres en ingles.

* **Piensa y analiza**: ¿los features en ingles aplican de la misma manera para los nombres en español?
"""

# escribe tu código aquí

"""2. **Entrenamiento y performance del modelo**: usando el classificador de Naive Bayes de NLTK entrena un modelo sencillo usando el mismo feature de la última letra del nombre, prueba algunas predicciones y calcula el performance del modelo."""

# escribe tu código aquí

"""3. **Mejores atributos:** Define una función como `atributos2()` donde puedas extraer mejores atributos con los cuales entrenar una mejor version del clasificador. Haz un segundo entrenamiento y verifica como mejora el performance de tu modelo. ¿Se te ocurren mejores maneras de definir atributos para esta tarea particular?"""

# escribe tu código aquí

"""# Clasificación de documentos (email spam o no spam)"""

!git clone https://github.com/pachocamacho1990/datasets

import pandas as pd
import numpy as np
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk import word_tokenize

df = pd.read_csv('datasets/email/csv/spam-apache.csv', names = ['clase','contenido'])
df['tokens'] = df['contenido'].apply(lambda x: word_tokenize(x)) #Aplicar un wordtokenize sobre cada fila
df.head()

df['tokens'].values[0] #Revisar la columna tokens

all_words = nltk.FreqDist([w for tokenlist in df['tokens'].values for w in tokenlist]) #Construir la frecuencia de palabras tomando la columna de tokens. Para que de cada tokenlist
top_words = all_words.most_common(200) #Buscar las palabras más frecuentes (top)

def document_features(document): #Función para crear los atributos
    document_words = set(document) #Crear un set con las palabras del documento
    features = {} #Crear un diccionario vacio
    for word in top_words: #Recorrer las palabras más frecuentes
        features['contains({})'.format(word)] = (word in document_words) #Verificar si las palabras más frecuentes estan
    return features

document_features(df['tokens'].values[0]) #Sobre la primera fila de tokens

fset = [(document_features(texto), clase) for texto, clase in zip(df['tokens'].values, df['clase'].values)]
random.shuffle(fset)
train, test = fset[:200], fset[200:]

classifier = nltk.NaiveBayesClassifier.train(train) #Llama al modelo

print(nltk.classify.accuracy(classifier, test))

classifier.show_most_informative_features(5)

df[df['clase']==-1]['contenido']

"""## Ejercicio de práctica

¿Como podrías construir un mejor clasificador de documentos?

0. **Dataset más grande:** El conjunto de datos que usamos fue muy pequeño, considera usar los archivos corpus que estan ubicados en la ruta: `datasets/email/plaintext/`

1. **Limpieza:** como te diste cuenta no hicimos ningun tipo de limpieza de texto en los correos electrónicos. Considera usar expresiones regulares, filtros por categorias gramaticales, etc ... .

---

Con base en eso construye un dataset más grande y con un tokenizado más pulido.
"""

from google.colab import drive
drive.mount('/content/drive')

# escribe tu código aquí:

"""2. **Validación del modelo anterior:**  
---

una vez tengas el nuevo conjunto de datos más pulido y de mayor tamaño, considera el mismo entrenamiento con el mismo tipo de atributos del ejemplo anterior, ¿mejora el accuracy del modelo resultante?
"""

# escribe tu código aquí:

"""3. **Construye mejores atributos**: A veces no solo se trata de las palabras más frecuentes sino de el contexto, y capturar contexto no es posible solo viendo los tokens de forma individual, ¿que tal si consideramos bi-gramas, tri-gramas ...?, ¿las secuencias de palabras podrián funcionar como mejores atributos para el modelo?. Para ver si es así,  podemos extraer n-gramas de nuestro corpus y obtener sus frecuencias de aparición con `FreqDist()`, desarrolla tu propia manera de hacerlo y entrena un modelo con esos nuevos atributos, no olvides compartir tus resultados en la sección de comentarios."""

# escribe tu código aquí: